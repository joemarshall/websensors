{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binary Classifier Training",
      "provenance": [],
      "authorship_tag": "ABX9TyOuIuZiMnGQjkbX7iN5drFT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joemarshall/websensors/blob/main/assets/python/BinaryClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab workbook presents a workflow for training a machine learning model for a simple binary classifier, then outputs it as a tflite file which can be used in the websensor platform or on a raspberry pi."
      ],
      "metadata": {
        "id": "y0x8G-ZpojrD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHMTFvXst49i"
      },
      "outputs": [],
      "source": [
        "# tensorflow is the machine learning library we use\n",
        "import tensorflow as tf\n",
        "# numpy is for fast python maths\n",
        "import numpy as np\n",
        "# pandas for importing datafiles\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# make some stuff that is in tensorflow be \n",
        "# easier to get at below\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow.keras.losses as losses\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load datafiles - each datafile is a csv file of continuous sensor data with. \n",
        "# accompanying ground truth tempo data calculated from the previous 4 beats\n",
        "\n",
        "# recorded data can be from a raspberry pi or from the websensor platform\n",
        "\n",
        "# this stuff makes an upload box appear\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "9lKzFRi3uriR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess - for each data point, add history of the previous 511 points\n",
        "# this is called 'unrolling'\n",
        "def unroll_data_and_preprocess(data,gt):\n",
        "  np_data=np.array(data)\n",
        "  np_data=(np_data/512.0) # scale the data so it isn't too big\n",
        "  return (np.lib.stride_tricks.sliding_window_view(np_data,window_shape=[512])).copy(),gt[511:]\n",
        "\n",
        "\n",
        "\n",
        "column_names=[\"sound level\",\"ground truth\"]\n",
        "\n",
        "datasets=[]\n",
        "\n",
        "for c in uploaded.keys():\n",
        "  print(f\"Loading: {c}\")\n",
        "  csv_frame=pd.read_csv(io.BytesIO(uploaded[c]))\n",
        "  file_x = csv_frame[column_names[0]].to_numpy()\n",
        "  file_y= csv_frame[column_names[1]].to_numpy()\n",
        "  datasets.append(unroll_data_and_preprocess(file_x,file_y))\n",
        "\n",
        "# make arrays for x and y\n",
        "x_data=np.concatenate([x for (x,y) in datasets])\n",
        "y_data=np.concatenate([y for (x,y) in datasets])\n",
        "print(f\"Loaded data: {x_data.shape},{y_data.shape}\")"
      ],
      "metadata": {
        "id": "9CWn7Q1f08hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xz34vn2b6uw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle the datasets\n",
        "p = np.random.permutation(x_data.shape[0])\n",
        "x_data=x_data[p]\n",
        "y_data=y_data[p]\n",
        "\n",
        "# split the datasets into train and test\n",
        "split_point=int (x_data.shape[0]*.75 )\n",
        "#split_point=x_data.shape[0]-1\n",
        "x_train=x_data[0:split_point]\n",
        "x_test=x_data[split_point:]\n",
        "y_train=y_data[0:split_point]\n",
        "y_test=y_data[split_point:]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WF2lnOzT7rRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build a model - 4 convolutional layers to identify features, then a fully connected layer to output \n",
        "# the tempo value\n",
        "model=keras.Sequential(layers=[layers.Input(name='x',shape=(512,1)),layers.Conv1D(32,kernel_size=3,padding=\"same\",strides=2,activation=\"relu\"),\n",
        "                         layers.Conv1D(32,kernel_size=3,padding=\"same\",strides=2,activation=\"relu\"),\n",
        "                         layers.Conv1D(32,kernel_size=3,padding=\"same\",strides=2,activation=\"relu\"),\n",
        "                         layers.Conv1D(32,kernel_size=3,padding=\"same\",strides=2,activation=\"relu\"),\n",
        "                         #layers.Conv1D(64,kernel_size=32,padding=\"same\",strides=32,activation=\"relu\"),\n",
        "                         layers.Flatten(),\n",
        "                         layers.Dense(64,activation=\"relu\"),\n",
        "                         layers.Dense(1,activation=\"sigmoid\",name='y')]) # sigmoid = between 0 and 1 (can either use this or 2 dimensions with softmax activation as the output layer)\n",
        "model.compile(optimizer='adam', loss='mse',run_eagerly=True) # mean squared error loss is a good default for regression problems\n",
        "model.build(input_shape=(None,512,1))\n",
        "model.summary()\n",
        "print(model.input,model.output)"
      ],
      "metadata": {
        "id": "vsYtumr07_2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call train on the model\n",
        "model.fit(x_train,y_train,batch_size=32,validation_data=(x_test,y_test),epochs=100)\n"
      ],
      "metadata": {
        "id": "MIAndjiv7-CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model to a tflite model for inference on raspberry pi (or websensor platform)\n",
        "converter=tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite=converter.convert()\n",
        "\n",
        "tflite_model_file = open('model.tflite',\"wb\")\n",
        "tflite_model_file.write(tflite)\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite)\n",
        "\n",
        "signatures = interpreter.get_signature_list()\n",
        "print(signatures)\n",
        "\n",
        "from google.colab import files\n",
        "files.download('model.tflite')\n"
      ],
      "metadata": {
        "id": "gGjERc-8Rwkz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
